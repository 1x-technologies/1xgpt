#!/usr/bin/env python3

import argparse
import time
from collections import defaultdict
from typing import Callable

import lpips
import numpy as np
import torch
import torchvision.transforms.functional as transforms_f
from einops import rearrange
from torch.utils.data import DataLoader, Subset
from tqdm import tqdm
from transformers import AutoModelForCausalLM, default_data_collator

from data import RawTokenDataset
from visualize import decode_latents_wrapper

# Hardcoded values for the final dataset
WINDOW_SIZE = 16
STRIDE = 15  # Data is 30 Hz so with stride 15, video is 2 Hz
LATENT_H, LATENT_W = 20, 20  # Dimensions of the compressed image


def parse_args():
    parser = argparse.ArgumentParser(description="")
    parser.add_argument(
        "--val_data_dir", type=str, default="data/val_v0",
        help="A directory with video data, should have a `metadata.json` and `video.bin`."
    )
    parser.add_argument(
        "--checkpoint_dir", type=str, default="1x-technologies/Llama_1B_v0",
        help="A directory with the model weights and config.json, or HF model."
    )
    parser.add_argument(
        "--batch_size", type=int, default=2,
        help="Batch size, current script only supports a single GPU."
    )

    return parser.parse_args()


class LlamaEvaluator:
    def __init__(self, checkpoint_dir: str, decode_latents: Callable, device="cuda"):
        super().__init__()
        self.model = AutoModelForCausalLM.from_pretrained(
            checkpoint_dir,
            torch_dtype=torch.bfloat16,
            attn_implementation="flash_attention_2"
        ).to(device)

        self.model.eval()

        self.decode_latents = decode_latents
        self.device = device

    def predict_zframe_logits(self, input_ids: torch.LongTensor) -> torch.Tensor:
        """
        Conditioned on previous frames: [frame_0], [frame_0, frame_1], ..., [frame_0, frame_1, ... frame_{T-1}],
        predict the tokens in the following frame: [pred_frame_1, pred_frame_2, ..., pred_frame_T].

        Image logits are auto-regressively generated across the spatial dimension (within an image) but teacher-forced
        across time dimension.

        Args:
            input_ids: LongTensor of size (B, T*H*W) corresponding to flattened, tokenized images.

        Returns:
            FloatTensor of size (B, C, T-1, H, W) corresponding to the predicted logits,
            where C=1000 is the number of classes.
        """
        input_ids = rearrange(input_ids, "b (t h w) -> b t h w", t=WINDOW_SIZE, h=LATENT_H, w=LATENT_W)

        all_logits = []
        for timestep in range(1, WINDOW_SIZE):
            prev_frames = rearrange(input_ids[:, :timestep, :, :].to(self.device), "b t h w -> b (t h w)")

            outputs = self.model.generate(
                input_ids=prev_frames, max_new_tokens=LATENT_H * LATENT_W, min_new_tokens=LATENT_H * LATENT_W,
                output_logits=True, return_dict_in_generate=True,
            )

            logits = torch.stack(outputs.logits, dim=1)
            all_logits.append(rearrange(logits, "b (h w) c -> b c h w", h=LATENT_H, w=LATENT_W))

        return torch.stack(all_logits, dim=2)

    def predict_zframe_logits_teacher(self, input_ids: torch.LongTensor) -> torch.Tensor:
        """
        Same task as `predict_zframe_logits`, except with teacher forcing.

        Returns:
            FloatTensor of size (B, C, T-1, H, W) corresponding to the predicted logits,
            where C=1000 is the number of classes.
        """

        logits = self.model(input_ids=input_ids.to(self.device)).logits  # (B, THW, C)

        shifted_logits = torch.cat([torch.zeros((logits.size(0), 1, logits.size(2)), device=self.device),
                                    logits[:, :-1, :]], dim=1)
        shifted_logits = rearrange(shifted_logits, "b (t h w) c -> b c t h w",
                                   t=WINDOW_SIZE, h=LATENT_H, w=LATENT_W)
        return shifted_logits[..., 1:, :, :]

    def predict_next_frames(self, input_ids=None, logits=None) -> torch.Tensor:
        """
        All model submissions should have this defined.

        Conditioned on each prefix: [frame_0], [frame_0, frame_1], ..., [frame_0, frame_1, ..., frame_{T-1}],
        predict the following frame: [pred_frame_1, pred_frame_2, ..., pred_frame_T].

        For this model, the frames are generated by using the argmax of `predict_zframe_logits`
        and decoding the quantized latent tokens back to the original image space.

        Args:
            input_ids: LongTensor of size (B, T*H*W) corresponding to flattened, tokenized images.
                If `logits` is not provided, will `predict_zframe_logits` using these `input_ids`.
            logits: optional FloatTensor of size (B, C, T-1, h, w).
                If `logits` has already been computed (e.g. when calculating loss),
                they can be specified here to avoid recomputation.

        Returns:
            LongTensor of size (B, T-1, 3, 160, 160) corresponding to the predicted frames.
        """
        if logits is None:
            assert input_ids is not None, "Expected at least one of `input_ids` or `logits` to be specified."
            logits = self.predict_zframe_logits(input_ids)

        top_preds = torch.argmax(logits, dim=1).cpu()
        return decode_tokens(top_preds, self.decode_latents)


def decode_tokens(reshaped_token_ids: torch.LongTensor, decode_latents: Callable) -> torch.ByteTensor:
    """
    Converts quantized latent space tokens to images.

    Args:
        reshaped_token_ids: shape (B, T, H, W).
        decode_latents: instance of `decode_latents_wrapper()`

    Returns:
        (B, T, 3, 160, 160)
    """
    decoded_imgs = decode_latents(rearrange(reshaped_token_ids, "b t h w -> (b t) h w").numpy())
    decoded_tensor = torch.stack([transforms_f.pil_to_tensor(pred_img) for pred_img in decoded_imgs])
    return rearrange(decoded_tensor, "(b t) c H W -> b t c H W", b=reshaped_token_ids.size(0))


def compute_loss_and_acc(input_ids: torch.LongTensor, logits: torch.FloatTensor) -> tuple[float, float]:
    """
    If applicable (Evaluator can return logits), compute the cross entropy loss and predicted token accuracy.

    Args:
        input_ids: LongTensor of size (B, T*H*W) corresponding to flattened, tokenized images.
        logits: FloatTensor of size (B, C, T-1, H, W). E.g. output of `LlamaEvaluator.predict_zframe_logits()`

    Returns:
        Cross entropy loss and predicted token accuracy.
    """

    assert logits.dim() == 5 and input_ids.size(0) == logits.size(0) and logits.size(1) == 1000, \
        "Shape of `logits` should be (B, C, T-1, h, w)"
    t = logits.size(2) + 1
    h, w = logits.size()[-2:]
    assert t * h * w == input_ids.size(1), "Shape of `logits` does not match flattened latent image size."
    input_ids = rearrange(input_ids, "b (t h w) -> b t h w", t=t, h=h, w=w)
    labels = input_ids[:, 1:].to(logits.device)
    top_preds = torch.argmax(logits, dim=1)
    return torch.nn.functional.cross_entropy(logits, labels).item(), (labels == top_preds).float().mean().item()


def compute_lpips(frames_a: torch.ByteTensor, frames_b: torch.ByteTensor, lpips_func: Callable) -> list:
    """
    Given two batches of video data, of shape (B, T, 3, 160, 160), computes the LPIPS score on frame-by-frame level.
    Cannot use `lpips_func` directly because it expects at most 4D input.
    """
    flattened_a, flattened_b = [rearrange(frames, "b t c H W -> (b t) c H W")
                                for frames in (frames_a, frames_b)]
    return lpips_func(flattened_a, flattened_b).flatten().tolist()


@torch.no_grad()
def main():
    args = parse_args()

    val_dataset = RawTokenDataset(args.val_data_dir, window_size=WINDOW_SIZE, stride=STRIDE)
    decode_latents = decode_latents_wrapper(unet_checkpoint_path=val_dataset.metadata["unet"])
    evaluator = LlamaEvaluator(args.checkpoint_dir, decode_latents)

    # To save time, only evaluate on each chunk once instead of using a sliding window.
    val_dataset = Subset(
        val_dataset,
        [i for chunk_start in range(val_dataset.stride)
         for i in range(chunk_start, len(val_dataset), val_dataset.stride * val_dataset.window_size)]
    )

    dataloader = DataLoader(val_dataset, collate_fn=default_data_collator, batch_size=args.batch_size,)

    lpips_alex = lpips.LPIPS(net="alex")  # Calculate LPIPS w/ AlexNet, which is the fastest model out of their options
    metrics = defaultdict(list)

    for i, batch in enumerate(tqdm(dataloader)):
        reshaped_input_ids = rearrange(batch["input_ids"], "b (t h w) -> b t h w", t=WINDOW_SIZE, h=LATENT_H)
        frames_per_batch = (WINDOW_SIZE - 1) * batch["input_ids"].size(0)

        start_time = time.time()
        logits = evaluator.predict_zframe_logits(batch["input_ids"])
        metrics["gen_time"].append((time.time() - start_time) / frames_per_batch)

        if logits is not None:
            loss, acc = compute_loss_and_acc(batch["input_ids"], logits)
            metrics["loss"].append(loss)
            metrics["acc"].append(acc)

        start_time = time.time()
        pred_frames = evaluator.predict_next_frames(logits=logits)
        metrics["dec_time"].append((time.time() - start_time) / frames_per_batch)

        logits_teacher = evaluator.predict_zframe_logits_teacher(batch["input_ids"])
        if logits_teacher is not None:
            loss_teacher, acc_teacher = compute_loss_and_acc(batch["input_ids"], logits_teacher)
            metrics["loss_teacher"].append(loss_teacher)
            metrics["acc_teacher"].append(acc_teacher)

        decoded_gtruth = decode_tokens(reshaped_input_ids, decode_latents)
        labels, prev_frames = decoded_gtruth[:, 1:], decoded_gtruth[:, :-1]
        redecoded_labels = decode_tokens(reshaped_input_ids[:, 1:], decode_latents)

        metrics["pred_lpips"].extend(compute_lpips(labels, pred_frames, lpips_alex))
        metrics["copy_lpips"].extend(compute_lpips(labels, prev_frames, lpips_alex))
        metrics["gtruth_lpips"].extend(compute_lpips(labels, redecoded_labels, lpips_alex))

        print({key: np.mean(val) for key, val in metrics.items()})


if __name__ == "__main__":
    main()
